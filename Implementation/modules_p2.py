# -*- coding: utf-8 -*-
"""modules_P2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1YNAMP52hQniVXNJk54P4NjdMFugMF2MT
"""

import tensorflow as tf
from tensorflow.keras.layers import *
from tensorflow.keras.regularizers import l2

def SE_Block(x):

  x_hat = GlobalAveragePooling2D()(x)
  x_hat = Dense(int(x_hat.shape[1]/4),activation='relu')(x_hat)
  x_hat = Dense(x_hat.shape[1]*4,activation='sigmoid')(x_hat)
  x = Multiply()([x_hat,x])

  return x

def SE_Res_Block(x,filters=32,kernel=(3,3),dilation_rate=(1,1),first=False,alfa=True,alfa_value=0.1):
  skip = x
  if first :
    skip = Conv2D(filters=filters,kernel_size=(3,3),padding='same',kernel_regularizer=l2(1e-3))(skip)
    # skip = BatchNormalization()(skip)

  x = Conv2D(filters=filters,kernel_size=kernel,padding='same',kernel_regularizer=l2(1e-3))(x)
  # x = BatchNormalization()(x)
  x = Activation('relu')(x)
  # x = Dropout(0.2)(x)

  x = Conv2D(filters=filters,kernel_size=kernel,padding='same',kernel_regularizer=l2(1e-3))(x)
  # x = BatchNormalization()(x)
  x = SE_Block(x)

  
  x = Add()([skip,x])

  return x

def SE_Bottleneck_Block(x,filters=32,kernel=(3,3),first=False,alfa=True,alfa_value=0.1):
  skip = x

  if first :
    skip = Conv2D(filters=filters,kernel_size=(3,3),strides=(2,2),padding='same',kernel_regularizer=l2(1e-3))(skip)
    # skip = BatchNormalization()(skip)

  
    
  x = Conv2D(filters=filters,kernel_size=kernel,strides=(2,2),padding='same',kernel_regularizer=l2(1e-3))(x)
  # x = BatchNormalization()(x)
  x = Activation('relu')(x)
  # x = Dropout(0.2)(x)

  x = Conv2D(filters=filters,kernel_size=kernel,padding='same',kernel_regularizer=l2(1e-3))(x)
  # x = BatchNormalization()(x)
  x = SE_Block(x)


  x = Add()([skip,x])

  return x

def self_attention(x1,x2):

  alfa = tf.Variable(initial_value=0,trainable=True,dtype=tf.float32)

  q = Conv2D(x1.shape[-1]/8,(1,1))(x1)
  q = Reshape((x1.shape[1]*x1.shape[2],int(x1.shape[-1]/8)))(q)
  k = Conv2D(x2.shape[-1]/8,(1,1))(x2)
  k = Reshape((x2.shape[1]*x2.shape[2],int(x2.shape[-1]/8)))(k)
  k = tf.transpose(k,perm=[0,2,1])
  qk = tf.linalg.matmul(q,k)
  # qk = tf.nn.softmax(qk,axis=(1,2))
  qk = tf.keras.activations.softmax(qk, axis=[1, 2])
  v = Conv2D(x2.shape[-1],(1,1))(x2)
  v = Reshape((x2.shape[1]*x2.shape[2],x2.shape[-1]))(v)
  x_hat = tf.linalg.matmul(qk,v)
  x_hat = Reshape((x2.shape[1],x2.shape[2],x2.shape[-1]))(x_hat)
  x_hat = alfa * x_hat
  x = Add()([x2,x_hat])
  return x
